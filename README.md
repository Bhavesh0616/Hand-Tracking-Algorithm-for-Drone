# âœ‹ğŸ¤– AI-Powered Gesture Control Drone

This project enables **hands-free control of drones** using real-time **hand gesture recognition**, **palm tracking**, and **gesture-based commands**. By integrating computer vision, machine learning, and MAVLink-based drone communication, the system lets users intuitively control drones using simple hand motions in front of a webcam.

> ğŸš€ Designed for next-gen HCI, drone innovation, and gesture-based robotics research.

---

## ğŸ“ Directory Structure

. â”œâ”€â”€ app.py â”œâ”€â”€ gesture_to_command.py â”œâ”€â”€ hand_coordinates.txt â”œâ”€â”€ keypoint_classification.ipynb â”œâ”€â”€ point_history_classification.ipynb â”‚ â”œâ”€â”€ model/ â”‚ â”œâ”€â”€ keypoint_classifier/ â”‚ â”‚ â”œâ”€â”€ keypoint.csv â”‚ â”‚ â”œâ”€â”€ keypoint_classifier.hdf5 â”‚ â”‚ â”œâ”€â”€ keypoint_classifier.tflite â”‚ â”‚ â”œâ”€â”€ keypoint_classifier.py â”‚ â”‚ â””â”€â”€ keypoint_classifier_label.csv â”‚ â”‚ â”‚ â””â”€â”€ point_history_classifier/ â”‚ â”œâ”€â”€ point_history.csv â”‚ â”œâ”€â”€ point_history_classifier.hdf5 â”‚ â”œâ”€â”€ point_history_classifier.tflite â”‚ â”œâ”€â”€ point_history_classifier.py â”‚ â””â”€â”€ point_history_classifier_label.csv â”‚ â””â”€â”€ utils/ â””â”€â”€ cvfpscalc.py


---

## ğŸ§  How It Works

### 1. **Hand Detection & Palm Tracking**
- Uses **MediaPipe** to detect and track 21 hand landmarks in real-time.
- Extracts the palm center (landmark `0`) for directional input.

### 2. **Gesture Recognition**
- Static gestures (e.g., âœ‹âœŠâ˜ï¸ğŸ‘Œ) classified using `keypoint_classifier.tflite`.
- Dynamic gestures (e.g., waving or pointing motions) processed with `point_history_classifier.tflite`.

### 3. **Drone Command Execution**
- The palm coordinates are written to `hand_coordinates.txt` by `app.py`.
- `gesture_to_command.py` reads these coordinates and sends drone commands using MAVLink:
  - Arm/disarm
  - Move left/right
  - Maintain hover altitude

---

## ğŸš¦ Key Components

### `app.py`
- Captures live webcam feed
- Detects hand, extracts gestures
- Logs coordinates to file for drone interaction

### `gesture_to_command.py`
- Connects to APM flight controller via COM port
- Arms drone, sets flight mode
- Converts gesture directions to MAVLink movement commands

---

## ğŸ“š Model Training

| Notebook | Purpose |
|----------|---------|
| `keypoint_classification.ipynb` | Train hand sign recognition using 21 landmarks |
| `point_history_classification.ipynb` | Train dynamic gesture recognition using finger trajectory history |

---

## ğŸ”§ Setup

### ğŸ”— Dependencies
- Python 3.8+
- MediaPipe
- OpenCV
- TensorFlow Lite
- pymavlink

Install dependencies:

pip install -r requirements.txt

ğŸ› ï¸ Running the System
Start gesture detection:

python app.py
Start drone controller interface:

python gesture_to_command.py

ğŸ’¡ Make sure your drone is connected (via COM3 or appropriate port) and set to GUIDED mode.

ğŸ’¥ Features
ğŸ”´ Real-time gesture and palm tracking

ğŸŸ¢ Gesture-to-drone command mapping

ğŸ”µ Modular, extendable ML architecture

ğŸŸ¡ Compatible with APM 2.8 / Pixhawk via MAVLink

ğŸŸ£ Training and logging tools included

ğŸ¯ Use Cases
Autonomous aerial filming for vloggers & content creators

Touch-free drone control for accessibility

Drone racing without physical controllers

Interactive robotics education

ğŸ¬ Demo

![Gesture Control Demo](./demo.gif")

![Gesture Control Demo](./demo_1.gif)

ğŸ‘¨â€ğŸ’» Contributors
You â€“ Core Developer, System Architect

ğŸ“„ License
This project is licensed under the MIT License.

ğŸ¤ Support
If you find this project helpful, feel free to give it a â­ on GitHub, open issues for bugs, or fork it for your own use!

yaml
Copy
Edit

---

Let me know if you want:
- A demo badge or GitHub Actions CI badge added
- A quick `requirements.txt` to go with it
- Help embedding demo images/GIFs

I can also generate a `LICENSE` file and `.gitignore` if you're publishing it as a complete repo.

